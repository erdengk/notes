## 机器学习是什么？

在进行特定编程的情况下，给予计算机学习能力的领域

目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习

监督学习这个想法是指，我们将教计算机如何去完成任务，

无监督学习中，我们打算让它自己进行学习

### 监督学习

![](http://www.ai-start.com/ml2014/images/2d99281dfc992452c9d32e022ce71161.png)

==监督学习==指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成

在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格

==回归==这个词的意思是，我们在试着推测出这一系列连续值属性。

**支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征**



### 无监督学习

![](http://www.ai-start.com/ml2014/images/0c93b5efd5fd5601ed475d2c8a0e6dcd.png)

![](http://www.ai-start.com/ml2014/images/94f0b1d26de3923fc4ae934ec05c66ab.png)

不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签

针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法。事实证明，它能被用在很多地方。

这是有一堆数据。我不知道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类型又是什么

## 单变量线性回归(Linear Regression with One Variable)

### 线性回归算法

在监督学习中我们有一个数据集，这个数据集被称==训练集==

<img src="https://raw.githubusercontent.com/erdengk/picGo/main/img/202208181555071.png" alt="image-20220818155533842" style="zoom:150%;" />

### 代价函数

代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

![](http://www.ai-start.com/ml2014/images/2c9fe871ca411ba557e65ac15d55745d.png)

通过计算代价函数，可以发现有一点能使得代价最低

为了找到函数最小值的算法，引入梯度下降

### 梯度下降

==梯度下降==是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值

梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），**选择不同的初始参数组合，可能会找到不同的局部最小值。**

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。



![image-20220818162856955](https://raw.githubusercontent.com/erdengk/picGo/main/img/202208181628001.png)

阿尔法 如果过大或过小，都会导致梯度下降过慢或者过快。

在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法



## 总结

通过不断试错，降低代价函数，也由此引入了梯度下降算法

如果下降过快、过慢都会导致错过最小代价

可能找到多个局部最优，然后多个局部最优再选出全局最优

