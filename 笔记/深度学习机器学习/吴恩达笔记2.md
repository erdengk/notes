## Week2

### 梯度下降的线性回归

### 梯度下降法实践1-特征缩放

很多时候，多个特征的单位不同，梯度下降算法需要非常多次的迭代才能收敛。

==解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间==

### 4.4 梯度下降法实践2-学习率

也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看上面这样的图表更好。

==相当于当误差小于某个值之后停止迭代，但通常选择一个合适的误差值会比较困难，所以推荐看图==

梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

### 4.5 特征和多项式回归

通常我们需要先观察数据然后再决定准备尝试怎样的模型

注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。

### 4.6 正规方程

<img src="https://raw.githubusercontent.com/erdengk/picGo/main/img/202208191622540.png" alt="image-20220819162236452" style="zoom:200%;" />

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。

随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，==像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。==因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。
