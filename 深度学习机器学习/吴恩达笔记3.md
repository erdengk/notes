## 六、逻辑回归(Logistic Regression)

在分类问题中，你要预测的变量`y`是离散的值，我们将学习一种叫做逻辑回归 (**Logistic Regression**) 的算法，这是目前最流行使用最广泛的一种学习算法。

==逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。==

逻辑回归算法实际上是一种分类算法，它适用于标签 `y` 取值离散的情况，如：1 0 0 1。

### 6.3 判定边界

下决策边界(**decision boundary**)的概念。这个概念能更好地帮助我们理解逻辑回归的假设函数在计算什么。

### 6.4 代价函数

如何拟合逻辑回归模型的参数

我要定义用来拟合参数的优化目标或者叫代价函数，这便是监督学习问题中的逻辑回归模型的拟合问题

![](http://www.ai-start.com/ml2014/images/f23eebddd70122ef05baa682f4d6bd0f.png)

除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：**共轭梯度**（**Conjugate Gradient**），**局部优化法**(**Broyden fletcher goldfarb shann,BFGS**)和**有限内存局部优化法**(**LBFGS**) ，**fminunc**是 **matlab**和**octave** 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导。

### 6.5 简化的成本函数和梯度下降

==如何运用梯度下降法，来拟合出逻辑回归的参数==

### 6.7 多类别分类：一对多

如何使用逻辑回归 (**logistic regression**)来解决多类别分类问题，具体来说，我想通过一个叫做"一对多" (**one-vs-all**) 的分类算法。

"一对余"方法

abc 三类，拆成 a（bc）、 （ab）c   、（ac）b  三个二元分类问题

## 七、正则化(Regularization)

### 7.1 过拟合的问题

过拟合(**over-fitting**)的问题

正则化(**regularization**)的技术，它可以改善或者减少过度拟合问题

问题是，如果我们发现了过拟合问题，应该如何处理？

1. 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如**PCA**）
2. 正则化。 保留所有的特征，但是减少参数的大小（**magnitude**）。



### 7.2 代价函数

正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，我们就能很好的拟合了

==引入惩罚系数，降低高次项系数==

假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度

### 7.3 正则化线性回归

正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令θ值减少了一个额外的值

### 7.4 正则化的逻辑回归模型

